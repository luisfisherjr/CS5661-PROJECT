{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/tf-source-keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "\n",
    "# import keras\n",
    "# from keras import models, layers, callbacks, constraints, backend, activations\n",
    "from keras.models import model_from_json\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# from matplotlib import pyplot\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# import datetime\n",
    "import pathlib\n",
    "import time\n",
    "# import json\n",
    "# import sys\n",
    "import os\n",
    "\n",
    "# %matplotlib inline\n",
    "# pd.options.display.max_columns = None\n",
    "\n",
    "from Utility import scale, ann_model, string_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/tf-source-keras/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  if __name__ == '__main__':\n",
      "/home/luis/anaconda3/envs/tf-source-keras/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.DataFrame.from_csv('../dataset/train.csv', index_col=None)\n",
    "test_dataset = pd.DataFrame.from_csv('../dataset/test.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split training into X_train, y_train & rename train_dataset into X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 55), (15120, 7), (15120,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.get_dummies(train_dataset['Cover_Type'],  prefix='Cover_Type_')\n",
    "y_single = train_dataset['Cover_Type']\n",
    "X_train = train_dataset.drop('Cover_Type', axis=1)\n",
    "\n",
    "X_train.shape, y_train.shape, y_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras ann bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ann params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "batch_size = 1\n",
    "\n",
    "hidden_layers = [\n",
    "    {'neurons': 100, 'dropout': 0.1 },\n",
    "    {'neurons': 100, 'dropout': 0.1 },\n",
    "    {'neurons': 100, 'dropout': 0.1 },\n",
    "    {'neurons': 100, 'dropout': 0.1 },\n",
    "]\n",
    "\n",
    "# activation = ['relu', 'softmax'],\n",
    "# optimizer = 'adam',\n",
    "# loss = 'categorical_crossentropy',\n",
    "\n",
    "noise = (True, 0.01)\n",
    "\n",
    "# metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of classifiers\n",
    "base_clf_count = 10\n",
    "sample_size = 0.6\n",
    "\n",
    "epochs = 100\n",
    "verbose = 0\n",
    "\n",
    "filepath = '../models'\n",
    "\n",
    "bootstarp_size = int(sample_size * X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next model suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_number = len(os.listdir(filepath))\n",
    "if model_number % 2 == 1:\n",
    "    model_number = model_number - 1\n",
    "model_number = int(model_number / 2)\n",
    "model_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate models for bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for i in range(base_clf_count): # loop for bagging\n",
    "\n",
    "    X_sample = resample(X_train_scaled, n_samples = bootstarp_size, replace=True) # sampling features\n",
    "    \n",
    "    y_sample = y_train.loc[X_sample.index] # get sample of labels with same index as sample of features\n",
    "    \n",
    "    model = ann_model(\n",
    "        features = features,\n",
    "        output_size = output_size,\n",
    "        batch_size = batch_size,\n",
    "        hidden_layers = hidden_layers,\n",
    "        noise = (True, 0.01),\n",
    "    )\n",
    "    \n",
    "    model.fit(X_sample, y_sample, verbose = verbose, epochs = epochs)\n",
    "\n",
    "    pathlib.Path(filepath).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open('{}/model_{}.json'.format(filepath, model_number), 'w') as json_file:\n",
    "        json_file.write(model.to_json()) # serialize model to JSON\n",
    "        json_file.close()\n",
    "    \n",
    "    model.save_weights('{}/model_{}.hdf5'.format(filepath, model_number))\n",
    "\n",
    "    model_number = model_number + 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print('time to train {} models : {}'.format(base_clf_count, string_time(start, end)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-source-keras]",
   "language": "python",
   "name": "conda-env-tf-source-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
